# Added args
wandb_project: default
gait_type: null
gpu_idx: 0

# existing args from pytorch_a2c_ppo_acktr_gail, defaults changed
algo: ppo
gail: False # do imitation learning with  gail
gail_experts_dir: ./gail_experts #directory that contains expert demonstrations for gail
gail_batch_size: 128
gail_epoch: 5
lr: 3.0e-5
eps: 1.0e-5 # RMSprop optimizer epsilon (default: 1e-5)
alpha: 0.99 # RMSprop optimizer apha (default: 0.99)
gamma: 0.99
use_gae: True
gae_lambda: 0.95
entropy_coef: 0.00
value_loss_coef: 0.5
max_grad_norm: 0.5
seed: 1
cuda_deterministic: False
num_processes: 20 
num_steps: 600
ppo_epoch: 10
num_mini_batch: 32
clip_param: 0.2
log_interval: 100
save_interval: 10
eval_interval: null
num_env_steps: 100_000_000
env_name: aliengo
wandb_log_interval: 1 #TODO
log_dir: /tmp/gym/
# save_dir: ./trained_models/
cuda: False
use_proper_time_limits: True
recurrent_policy: False
use_linear_lr_decay: False
num_torch_threads: null # null just means don't set anything.


env_params:
  obstacles: null
  render: False
  # env_mode: pmtg
  apply_perturb: False
  avg_time_per_perturb: 5.0 # seconds
  action_repeat: 4 # TODO change back to 6
  timeout: 60.0 # number of seconds to timeout after
  # flat_ground: True # this is for getting terrain scan in privileged info for Aliengo 
  vis: False
  max_torque: 44.4 # from URDF 
  kp: 1.0  
  kd: 1.0 
  fixed: False 
  fixed_position: [0,0,1.0] #TODO remove
  fixed_orientation: [0,0,0] #TODO remove
  gait_type: trot
  observation_parts: #TODO enumerate all possible types that I can even put here. Also add terrain heightmaps
    - joint_torques
    - joint_positions
    - joint_velocities
    - base_angular_velocity
    - base_roll
    - base_pitch
    # - base_linear_acceleration #TODO

  # possible values ['joint_positions', 'joint_velocities', 'joint_torques'] 
  # eventually add pmtg and footstep_param stuff
  # null means to use the widest bounds allowed ie no additional restriction imposed
  action_space: 
    joint_positions: #TODO implement the bounds and other action spaces.
      lb: null
      ub: null
  
  reward_parts: #TODO
    # reward_type: coefficient
    # note: the reward bounds are on the measured value of the thing, not on the reward term
    forward_velocity: [1.0, -1.5, 1.5] #coeffient, lower bound, upper bound
    joint_torques_l2: [-0.000005]
